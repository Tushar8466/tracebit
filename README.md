## AI-GENERATED CONTRIBUTION DETECTOR FOR GITHUB

## 1. ğŸ§© Problem Statement

### Problem Title
AI-Generated Contribution Detector for GitHub

### Problem Description
The rise of AI coding tools like GitHub Copilot, ChatGPT, and similar assistants has made it
increasingly easy for developers to submit AI-generated code as their own contributions on GitHub.
While AI assistance is not inherently wrong, the lack of transparency around it raises serious
concerns in academic settings, hiring assessments, open-source integrity, and collaborative
projects. Currently, there is no reliable, automated mechanism on GitHub or elsewhere that can
flag or identify whether a commit, pull request, or code snippet was written by a human or
generated by an AI â€” making it nearly impossible for reviewers, educators, and maintainers to
make informed judgments about contribution authenticity.

### Target Users
â€¢â   â ğŸ“ *Educators & Academic Institutions* â€” detecting AI-written code in student assignments
  submitted via GitHub
â€¢â   â ğŸ¢ *Recruiters & Hiring Managers* â€” verifying that coding assessments and portfolio projects
  reflect genuine candidate skills
â€¢â   â ğŸŒ *Open-Source Maintainers* â€” ensuring contributions to repositories are human-authored
  and thoughtfully written
â€¢â   â ğŸ¤ *Team Leads & Code Reviewers* â€” maintaining transparency and accountability in
  collaborative codebases
â€¢â   â ğŸ“‹ *Compliance & Audit Teams* â€” organizations requiring human-verified code in regulated
  environments

### Existing Gaps
â€¢â   â âŒ *No Native GitHub Support* â€” GitHub provides no built-in tooling to detect or label
  AI-generated commits or pull requests
â€¢â   â âŒ *General-purpose detectors fall short* â€” Tools like GPTZero or Copyleaks are designed
  for natural language text, not code, and perform poorly on programming languages
â€¢â   â âŒ *No commit-level granularity* â€” Existing solutions (if any) work on file level, not at
  the granularity of individual commits, diffs, or pull requests
â€¢â   â âŒ *No contextual analysis* â€” Current tools ignore coding style consistency, commit message
  patterns, authorship history, and repository context â€” all strong signals of AI generation
â€¢â   â âŒ *Lack of integration* â€” There is no CI/CD-friendly or GitHub Actions-compatible tool that
  can automatically scan contributions during the review pipeline
â€¢â   â âŒ *No explainability* â€” Even partial solutions provide a binary result with no reasoning,
  making it hard for reviewers to act on the output

## 2. ğŸ” Problem Understanding & Approach

### Root Cause Analysis

The core issue stems from a *fundamental shift in how code is being written* â€” AI tools have
lowered the barrier to producing syntactically correct, functional code without requiring deep
understanding. The root causes can be broken down as follows:

#### ğŸ”¸ 1. Invisibility of AI Involvement
AI-generated code looks structurally identical to human-written code on the surface. GitHub's
contribution system only tracks what was committed, never how it was written â€” creating a
blind spot in the entire contribution pipeline.

#### ğŸ”¸ 2. Absence of Behavioral Signals in Reviews
Human developers leave subtle fingerprints â€” inconsistent variable naming, iterative commit
history, personal coding style, typos and corrections. AI-generated code, by contrast, tends
to be overly clean, uniformly structured, and committed in large single chunks. These behavioral
differences are never captured or analyzed.

#### ğŸ”¸ 3. Misaligned Incentive Structures
In academic, hiring, and open-source contexts, contributors are rewarded for output (working code)
rather than process (how it was built). This incentivizes AI-assisted shortcuts with no
accountability mechanism to balance it.

#### ğŸ”¸ 4. No Domain-Specific Detection Models
Existing AI content detectors are trained on prose and natural language. Code has a fundamentally
different structure â€” syntax, logic flow, token patterns, and entropy distributions â€” that generic
NLP-based detectors are not equipped to analyze accurately.

#### ğŸ”¸ 5. Lack of Contextual Awareness
A single code snippet in isolation is hard to classify. Without comparing it against the author's
historical contributions, commit frequency, coding style, and repository patterns, any detection
attempt is shallow and unreliable.

---

### Solution Strategy

The strategy is to build a *multi-signal, context-aware detection system* that analyzes GitHub
contributions at the commit and pull request level using a combination of static code analysis,
behavioral pattern recognition, and a trained ML/AI classification model.

#### ğŸ”¹ 1. Multi-Signal Feature Extraction
Rather than relying on a single indicator, the system extracts multiple signals:
â€¢â   â *Code-level signals* â€” token entropy, cyclomatic complexity, comment-to-code ratio,
  naming convention uniformity, code structure patterns
â€¢â   â *Commit-level signals* â€” commit size, time between commits, message quality and length,
  frequency of single large dumps vs. incremental changes
â€¢â   â *Author-level signals* â€” deviation from the author's historical coding style and
  contribution behavior across their GitHub profile

#### ğŸ”¹ 2. Trained Classification Model
A supervised ML model (or fine-tuned code LLM) will be trained on a labeled dataset of
human-written vs. AI-generated code contributions to learn discriminative patterns specific
to code â€” not natural language.

#### ğŸ”¹ 3. GitHub Integration via API & Actions
The detector will integrate directly with GitHub using the GitHub REST API and GitHub Actions,
enabling automated scanning of pull requests and commits during the review process â€” no manual
effort required from maintainers.

#### ğŸ”¹ 4. Explainable Output
Instead of a black-box verdict, the system will return a *confidence score* alongside
highlighted signals that contributed to the decision, giving reviewers actionable, interpretable
results.

#### ğŸ”¹ 5. Continuous Improvement Loop
As AI coding tools evolve, so will their outputs. The system is designed with a feedback
mechanism that allows flagged contributions to be reviewed and used to retrain and improve
the model over time.

## 3. ğŸ’¡ Proposed Solution

### Solution Overview

*AI-Generated Contribution Detector for GitHub* is an intelligent analysis platform that
automatically scans GitHub commits, pull requests, and code contributions to determine whether
the code was written by a human or generated by an AI tool. It integrates seamlessly into
existing GitHub workflows via GitHub Actions and a web dashboard, providing developers,
educators, recruiters, and open-source maintainers with a transparent, explainable, and
real-time detection report for every contribution â€” without disrupting the development process.

---

### Core Idea

	â *"Don't just look at the code â€” look at how it was written."*

The central innovation is shifting detection from *what the code does* to **how it behaves
as a contribution**. AI-generated code is not just syntactically different â€” it leaves
behind a distinct behavioral signature in the way it is structured, committed, and styled.
By combining static code analysis, author behavioral profiling, and a domain-specific
ML classification model trained exclusively on code, the system builds a **multi-dimensional
fingerprint** of every contribution and compares it against known human and AI patterns to
produce an accurate, confidence-scored verdict.

---

### Key Features

#### ğŸ” 1. Commit & Pull Request Scanner
Automatically scans individual commits and pull requests on GitHub in real time. Supports
scanning by repository URL, PR link, or direct code diff input through the web interface
or API.

#### ğŸ§  2. AI vs. Human Classification Model
A purpose-built ML model trained on labeled datasets of human-written and AI-generated
code (from tools like GitHub Copilot, ChatGPT, Gemini, etc.) that identifies subtle
statistical and structural patterns unique to AI-generated code across multiple programming
languages.

#### ğŸ“Š 3. Confidence Score & Explainability Report
Every scan returns a *confidence score (0â€“100%)* indicating the likelihood of AI
generation, along with a breakdown of the contributing signals â€” such as token entropy
spikes, uniform naming patterns, commit size anomalies, and style inconsistencies â€” so
reviewers understand why a contribution was flagged.

#### ğŸ‘¤ 4. Author Behavioral Profiling
Builds a baseline coding profile for each GitHub user by analyzing their historical
contributions â€” commit frequency, code style, complexity trends â€” and flags deviations
that suggest AI assistance, adding a personalized layer of context to every detection.

#### âš™ï¸ 5. GitHub Actions Integration
Provides a plug-and-play *GitHub Actions workflow* that can be added to any repository
with a single configuration file. Automatically triggers scans on every new pull request
and posts the detection report as a PR comment for reviewers to see instantly.

#### ğŸŒ 6. Web Dashboard
A clean, interactive web interface where users can paste code, enter a GitHub repository
or PR URL, and instantly receive a detailed detection report â€” no setup required. Includes
scan history, repository-level analytics, and contributor risk summaries.

#### ğŸ”— 7. REST API Access
A fully documented REST API that allows teams to integrate the detector into their own
CI/CD pipelines, LMS platforms, recruitment tools, or internal review systems
programmatically.

#### ğŸ”„ 8. Feedback & Retraining Loop
Reviewers can mark detections as correct or incorrect directly from the dashboard or PR
comment. This feedback is collected and used to continuously retrain and improve the
model as AI coding tools evolve.

#### ğŸŒ 9. Multi-Language Support
Supports detection across the most widely used programming languages on GitHub including
Python, JavaScript, TypeScript, Java, C++, Go, and Rust â€” with plans to expand coverage
based on usage data.

#### ğŸ”’ 10. Privacy-First Design
No source code is stored permanently. All scans are processed ephemerally, and only
anonymized feature vectors are retained for model improvement â€” ensuring contributor
privacy and organizational complia


## 8. ğŸ› ï¸ Technology Stack

| Layer          | Technology                                      |
|----------------|-------------------------------------------------|
| *Frontend*   | React.js, Tailwind CSS         |
| *Backend*    | Node.js, Express.js    |
| *ML/AI*      | Python, FastAPI, PyTorch |
| *Database*   | MongoDB (Mongoose ODM), MongoDB Atlas           |
| *Deployment* | Vercel (Frontend), Render / Railway (ML API), GitHub Actions (CI/CD) |

## 9. Links
- Google Drive: https://drive.google.com/drive/folders/166F_ocIBV1FFcYvpqdzpZYmdH6Dq2j36?usp=sharing
- Github: https://github.com/Tushar8466/tracebit
- Demo Link: https://tracebit-jssg-frontend.vercel.app/
